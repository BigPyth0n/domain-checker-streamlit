import streamlit as st
import random
import whois
import json
import datetime
import os
from typing import List, Tuple
import pandas as pd

# --- ØªÙˆØ§Ø¨Ø¹ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª ---


def save_words_to_json(word_list: List[str], filename: str):
    try:
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(word_list, f, indent=4, ensure_ascii=False)
        st.sidebar.success(
            f"Successfully saved {len(word_list)} words to {filename}.")
    except Exception as e:
        st.sidebar.error(f"Error saving to {filename}: {str(e)}")


def load_words_from_json(filename: str) -> List[str]:
    try:
        with open(filename, "r", encoding="utf-8") as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return []

# ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ÛŒ Ú©Ù„Ù…Ø§Øª Ù…Ù†ØªØ®Ø¨


def load_curated_list(filename: str) -> List[str]:
    words = load_words_from_json(filename)
    if not words:
        st.sidebar.warning(f"ÙØ§ÛŒÙ„ '{filename}' ÛŒØ§ÙØª Ù†Ø´Ø¯ ÛŒØ§ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª.")
    return words


# --- Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù¾Ø³ÙˆÙ†Ø¯Ù‡Ø§ Ùˆ Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§ ---
domain_extensions_data = {
    1: (".nl", "6897290"), 2: (".info", "4927230"), 3: (".xyz", "3605402"),
    4: (".in", "13264020"), 5: (".at", "12130549"), 6: (".online", "3557169"),
    7: (".me", "10699054"), 8: (".vip", "16929713"), 9: (".live", "4039497"),
    10: (".trade", "6692301"), 11: (".cc", "9960073"), 12: (".ir", "450000"),
    13: (".mobi", "7114338"), 14: (".express", "13505184"), 15: (".io", "46725525"),
    16: (".icu", "4482844"), 17: (".work", "10418285"), 18: (".zone", "13505184"),
    19: (".site", "2351349"), 20: (".life", "3014550"), 21: (".space", "2351349"),
    22: (".club", "16929713"), 23: (".ltd", "8983359"), 24: (".link", "9067766"),
    25: (".works", "9887724"), 26: (".pro", "4721929"), 27: (".asia", "14325142"),
    28: (".app", "19244887"), 29: (".rocks", "5064444"), 30: (".tech", "4762989"),
    31: (".plus", "13505184"), 32: (".cloud", "18666094"), 33: (".biz", "20836570"),
    34: (".dev", "16495618"), 35: (".observer", "11394999"),
    37: (".direct", "18027009"), 38: (".store", "3557169"), 39: (".solutions", "9887724"),
    40: (".shop", "2399582"), 47: (".website", "2351349"), 48: (".today", "4039497"),
    49: (".name", "8296042"), 50: (".guru", "4039497"), 51: (".show", "16218279"),
    52: (".london", "36488113"), 53: (".design", "50933837"), 55: (".center", "10792089"),
    56: (".news", "13505184"), 57: (".tel", "11202068"), 58: (".win", "6692301"),
    59: (".email", "6089391"), 60: (".company", "8139285"), 61: (".systems", "24357564"),
    62: (".li", "8513089"), 63: (".host", "97671420"), 64: (".one", "17303517"),
    65: (".loan", "6692301"), 66: (".business", "4039497"), 67: (".world", "3014550"),
    68: (".click", "2399582"), 70: (".ink", "26045712"), 71: (".market", "40768774"),
    72: (".tv", "31110156"), 73: (".bid", "6692301"), 74: (".stream", "6692301"),
    76: (".men", "6692301"), 77: (".pw", "7174629"), 78: (".photography", "13505184"),
    79: (".exchange", "13505184"), 81: (".ph", "54985392"), 82: (".buzz", "33642378"),
    83: (".international", "13505184"), 84: (".media", "8139285"),
    86: (".berlin", "55720942"), 87: (".jobs", "198960300"), 90: (".group", "9887724"),
    91: (".global", "36114309"), 92: (".art", "3574321"), 93: (".blog", "4762989"),
    94: (".studio", "19835739"), 95: (".ooo", "27480638"), 97: (".digital", "3014550"),
    98: (".services", "10792089"), 99: (".expert", "10792089"), 100: (".tools", "16218279"),
    101: (".agency", "8139285"),
    102: (".network", "8139285")
}
temp_extensions = {}
seen_tlds = set()
for key, (tld, price_val) in domain_extensions_data.items():
    if tld not in seen_tlds:
        temp_extensions[key] = (tld, price_val)
        seen_tlds.add(tld)
domain_extensions = temp_extensions


# --- ØªÙˆØ§Ø¨Ø¹ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ ---
def price_to_number(price: str) -> float:
    price_clean = price.replace(",", "").strip()
    try:
        return float(price_clean)
    except ValueError:
        return float('inf')


def generate_random_words(count: int, length: int) -> List[str]:
    letters = "abcdefghijklmnopqrstuvwxyz"
    words = set()
    if count == 0:
        return []
    progress_bar = st.sidebar.progress(0)
    status_text = st.sidebar.empty()
    for i in range(count * 2):
        word = ''.join(random.choice(letters) for _ in range(length))
        words.add(word)
        progress = min((i + 1) / (count * 1.5), 1.0)
        progress_bar.progress(progress)
        status_text.text(
            f"ØªÙˆÙ„ÛŒØ¯ Ú©Ù„Ù…Ù‡ ØªØµØ§Ø¯ÙÛŒ {length} Ø­Ø±ÙÛŒ: {len(words)}/{count}")
        if len(words) >= count:
            break
    progress_bar.empty()
    status_text.empty()
    return list(words)[:count]


def check_domain_availability(domain: str) -> str:
    try:
        w = whois.whois(domain)
        if w is None or not hasattr(w, 'status'):
            return "Available"
        if w.status is None or \
           (isinstance(w.status, list) and any("available" in str(s).lower() for s in w.status)) or \
           (isinstance(w.status, str) and "available" in w.status.lower()) or \
           (hasattr(w, 'text') and ("no match" in str(w.text).lower() or "not found" in str(w.text).lower())) or \
           (hasattr(w, 'registrar') and w.registrar is None and w.creation_date is None) or \
           w.expiration_date is None or \
           (isinstance(w.expiration_date, list) and not w.expiration_date):
            return "Available"
        else:
            return "Registered"
    except whois.parser.PywhoisError as e:
        if "no match for" in str(e).lower() or "no entries found" in str(e).lower():
            return "Available"
        return f"Error: WHOIS Lookup ({str(e)})"
    except Exception as e:
        common_errors = ["no whois server", "no match", "not found",
                         "failed to connect", "timed out", "network is unreachable"]
        if any(err_msg in str(e).lower() for err_msg in common_errors):
            return "Available"
        return f"Error: General ({str(e)})"


def save_results_to_json(domains_df: pd.DataFrame,
                         domain_extension: str,
                         price_val: str,
                         list_type_identifier: str) -> str:
    filename = f"available_domains_{list_type_identifier}_{domain_extension.replace('.', '_')}.json"

    available_domains = domains_df[domains_df["ÙˆØ¶Ø¹ÛŒØª"] == "Available"]
    if available_domains.empty:
        st.warning(
            f"Ù‡ÛŒÚ† Ø¯Ø§Ù…Ù†Ù‡ Ø¢Ø²Ø§Ø¯ Ø¬Ø¯ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù„ÛŒØ³Øª '{list_type_identifier}' Ùˆ Ù¾Ø³ÙˆÙ†Ø¯ '{domain_extension}' ÛŒØ§ÙØª Ù†Ø´Ø¯. ÙØ§ÛŒÙ„ '{filename}' Ø¨Ù‡â€ŒØ±ÙˆØ² Ù†Ø´Ø¯."
        )
        return filename

    try:
        with open(filename, "r", encoding="utf-8") as f:
            existing_data = json.load(f)
            existing_domain_names = {item["domain"] for item in existing_data}
    except (FileNotFoundError, json.JSONDecodeError):
        existing_data = []
        existing_domain_names = set()

    new_domains_to_save = []
    for index, row in available_domains.iterrows():
        full_domain_name = row["Ø¯Ø§Ù…Ù†Ù‡ Ú©Ø§Ù…Ù„"]
        if full_domain_name not in existing_domain_names:
            new_domains_to_save.append({
                "domain": full_domain_name,
                "checked_at": datetime.datetime.now().isoformat(),
                "price": price_val
            })

    if new_domains_to_save:
        all_data = existing_data + new_domains_to_save
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(all_data, f, indent=4,
                      sort_keys=True, ensure_ascii=False)
        st.success(
            f"{len(new_domains_to_save)} Ø¯Ø§Ù…Ù†Ù‡ Ø¢Ø²Ø§Ø¯ Ø¬Ø¯ÛŒØ¯ Ø¯Ø± ÙØ§ÛŒÙ„ '{filename}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯. Ù…Ø¬Ù…ÙˆØ¹Ø§Ù‹ {len(all_data)} Ø¯Ø§Ù…Ù†Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª."
        )
    else:
        st.info(
            f"Ù‡ÛŒÚ† Ø¯Ø§Ù…Ù†Ù‡ Ø¢Ø²Ø§Ø¯ *Ø¬Ø¯ÛŒØ¯ÛŒ* Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ù„ÛŒØ³Øª '{list_type_identifier}' Ø¨Ø§ Ù¾Ø³ÙˆÙ†Ø¯ '{domain_extension}' ÛŒØ§ÙØª Ù†Ø´Ø¯. ÙØ§ÛŒÙ„ '{filename}' Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø¨Ø§Ù‚ÛŒ Ù…Ø§Ù†Ø¯."
        )
    return filename


def main():
    st.set_page_config(layout="wide", page_title="Ø§Ø¨Ø²Ø§Ø± Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø§Ù…Ù†Ù‡")
    st.title("Ø§Ø¨Ø²Ø§Ø± Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø¨ÙˆØ¯Ù† Ø¯Ø§Ù…Ù†Ù‡ ğŸ”")

    st.sidebar.title("ğŸ“š ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª")
    word_source_options = {
        "ØªØµØ§Ø¯ÙÛŒ": "ØªÙˆÙ„ÛŒØ¯ Ú©Ù„Ù…Ø§Øª ØªØµØ§Ø¯ÙÛŒ",
        "Ù…Ù†ØªØ®Ø¨ Ø·ÙˆÙ„Ø§Ù†ÛŒ": "Ù„ÛŒØ³Øª Ù…Ù†ØªØ®Ø¨ (Ø¨ÛŒØ´ Ø§Ø² Û¹Û°Û° Ú©Ù„Ù…Ù‡)",
        "Ù…Ù†ØªØ®Ø¨ ÛµÛ°Û°": "Ù„ÛŒØ³Øª Ù…Ù†ØªØ®Ø¨ Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡ (ÛµÛ°Û° Ú©Ù„Ù…Ù‡)",
        # <-- Ú¯Ø²ÛŒÙ†Ù‡ Ø¬Ø¯ÛŒØ¯ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
        "Ù…Ù†ØªØ®Ø¨ Û±Û°Û° Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡": "Ù„ÛŒØ³Øª Ù…Ù†ØªØ®Ø¨ Û±Û°Û° Ú©Ù„Ù…Ù‡ (ÙˆÛŒÚ˜Ù‡)"
    }
    word_source_key = st.sidebar.radio(
        "Ù…Ù†Ø¨Ø¹ Ú©Ù„Ù…Ø§Øª Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø§Ù…Ù†Ù‡ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯:",
        options=list(word_source_options.keys()),
        format_func=lambda key: word_source_options[key],
        key="word_source_selector"  # Added key for uniqueness
    )

    words_to_check_from_source = []
    list_type_id_for_save = ""

    if word_source_key == "ØªØµØ§Ø¯ÙÛŒ":
        list_type_id_for_save = "random"
        st.sidebar.subheader("âš™ï¸ ØªÙ†Ø¸ÛŒÙ…Ø§Øª ØªÙˆÙ„ÛŒØ¯ Ú©Ù„Ù…Ø§Øª ØªØµØ§Ø¯ÙÛŒ")
        word_count_for_dict = st.sidebar.number_input(
            "ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ ØªØµØ§Ø¯ÙÛŒ (Ù…Ø«Ù„Ø§Ù‹ Û±Û°Û°):",
            min_value=0, value=10, step=10,
            key="random_word_count_dict"  # Added key
        )
        if word_count_for_dict > 0:
            if st.sidebar.button("ØªÙˆÙ„ÛŒØ¯ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ÛŒ ØªØµØ§Ø¯ÙÛŒ Ø¬Ø¯ÛŒØ¯", key="gen_random_btn"):
                three_letter_words = generate_random_words(
                    word_count_for_dict, 3)
                four_letter_words = generate_random_words(
                    word_count_for_dict, 4)
                save_words_to_json(three_letter_words,
                                   "three_letter_words.json")
                save_words_to_json(four_letter_words, "four_letter_words.json")
            else:
                three_letter_words = load_words_from_json(
                    "three_letter_words.json")
                four_letter_words = load_words_from_json(
                    "four_letter_words.json")
                if not three_letter_words and not four_letter_words:
                    st.sidebar.caption(
                        "Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ÛŒ ØªØµØ§Ø¯ÙÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³ØªÙ†Ø¯. Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ØŒ Ø¯Ú©Ù…Ù‡ Ø¨Ø§Ù„Ø§ Ø±Ø§ Ø¨Ø²Ù†ÛŒØ¯.")
                else:
                    st.sidebar.caption(
                        f"Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡: {len(three_letter_words)} Ú©Ù„Ù…Ù‡ Û³Ø­Ø±ÙÛŒØŒ {len(four_letter_words)} Ú©Ù„Ù…Ù‡ Û´Ø­Ø±ÙÛŒ")

    elif word_source_key == "Ù…Ù†ØªØ®Ø¨ Ø·ÙˆÙ„Ø§Ù†ÛŒ":
        list_type_id_for_save = "curated_900_plus"
        words_to_check_from_source = load_curated_list(
            "curated_ai_trade_words.json")
        if words_to_check_from_source:
            st.sidebar.info(
                f"Ù„ÛŒØ³Øª Ù…Ù†ØªØ®Ø¨ (Ø¨ÛŒØ´ Ø§Ø² Û¹Û°Û° Ú©Ù„Ù…Ù‡) Ø´Ø§Ù…Ù„ {len(words_to_check_from_source)} Ú©Ù„Ù…Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯."
            )

    elif word_source_key == "Ù…Ù†ØªØ®Ø¨ ÛµÛ°Û°":
        list_type_id_for_save = "curated_500"
        words_to_check_from_source = load_curated_list(
            "curated_500_ai_trade_words.json")
        if words_to_check_from_source:
            st.sidebar.info(
                f"Ù„ÛŒØ³Øª Ù…Ù†ØªØ®Ø¨ Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡ (ÛµÛ°Û° Ú©Ù„Ù…Ù‡) Ø´Ø§Ù…Ù„ {len(words_to_check_from_source)} Ú©Ù„Ù…Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯."
            )

    elif word_source_key == "Ù…Ù†ØªØ®Ø¨ Û±Û°Û° Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡":  # <-- Ø¨Ù„Ø§Ú© Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ù„ÛŒØ³Øª Û±Û°Û° Ú©Ù„Ù…Ù‡â€ŒØ§ÛŒ
        list_type_id_for_save = "curated_100_creative"
        words_to_check_from_source = load_curated_list(
            "curated_100_ai_trade_words.json")
        if words_to_check_from_source:
            st.sidebar.info(
                f"Ù„ÛŒØ³Øª Ù…Ù†ØªØ®Ø¨ Û±Û°Û° Ú©Ù„Ù…Ù‡ (ÙˆÛŒÚ˜Ù‡) Ø´Ø§Ù…Ù„ {len(words_to_check_from_source)} Ú©Ù„Ù…Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯."
            )

    st.header("ğŸŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù¾Ø³ÙˆÙ†Ø¯ Ø¯Ø§Ù…Ù†Ù‡")
    sort_option = st.selectbox("Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø³ÙˆÙ†Ø¯Ù‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‚ÛŒÙ…Øª:", [
                               "Ø¨Ø¯ÙˆÙ† Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ", "ØµØ¹ÙˆØ¯ÛŒ", "Ù†Ø²ÙˆÙ„ÛŒ"], key="tld_sort_option")

    sorted_extensions_dict = domain_extensions.copy()
    if sort_option == "ØµØ¹ÙˆØ¯ÛŒ":
        sorted_extensions_dict = dict(
            sorted(domain_extensions.items(),
                   key=lambda item: price_to_number(item[1][1]))
        )
    elif sort_option == "Ù†Ø²ÙˆÙ„ÛŒ":
        sorted_extensions_dict = dict(
            sorted(domain_extensions.items(),
                   key=lambda item: price_to_number(item[1][1]), reverse=True)
        )

    selectbox_options = list(sorted_extensions_dict.items())
    selected_key_id = st.selectbox(
        "ÛŒÚ© Ù¾Ø³ÙˆÙ†Ø¯ Ø¯Ø§Ù…Ù†Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯:",
        options=[item[0] for item in selectbox_options],
        format_func=lambda key_id_func: f"{sorted_extensions_dict[key_id_func][0]} - Ù‚ÛŒÙ…Øª: {sorted_extensions_dict[key_id_func][1]}",
        key="tld_selector"
    )
    domain_extension_val, price_val = sorted_extensions_dict[selected_key_id]

    if word_source_key == "ØªØµØ§Ø¯ÙÛŒ":
        st.header("ğŸ”¢ ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø§Ù…Ù†Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ (Ø§Ø² Ù„ÛŒØ³Øª ØªØµØ§Ø¯ÙÛŒ)")
        three_letter_count_to_check = st.number_input(
            "ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø§Ù…Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Û³ Ø­Ø±ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ:", min_value=0, value=5, step=5, key="three_check_count"
        )
        four_letter_count_to_check = st.number_input(
            "ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø§Ù…Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Û´ Ø­Ø±ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ:", min_value=0, value=5, step=5, key="four_check_count"
        )

    if st.button("ğŸš€ Ø´Ø±ÙˆØ¹ Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø§Ù…Ù†Ù‡â€ŒÙ‡Ø§", type="primary", use_container_width=True, key="start_check_button"):
        final_words_to_process = []

        if word_source_key == "ØªØµØ§Ø¯ÙÛŒ":
            three_loaded = load_words_from_json("three_letter_words.json")
            four_loaded = load_words_from_json("four_letter_words.json")
            if 'three_letter_count_to_check' in locals() and three_letter_count_to_check > 0 and three_loaded:
                final_words_to_process.extend(
                    random.sample(three_loaded, min(
                        three_letter_count_to_check, len(three_loaded)))
                )
            if 'four_letter_count_to_check' in locals() and four_letter_count_to_check > 0 and four_loaded:
                final_words_to_process.extend(
                    random.sample(four_loaded, min(
                        four_letter_count_to_check, len(four_loaded)))
                )

            # Check if counts were defined and if any words were added
            random_counts_defined = 'three_letter_count_to_check' in locals(
            ) and 'four_letter_count_to_check' in locals()
            if not final_words_to_process and random_counts_defined and (three_letter_count_to_check > 0 or four_letter_count_to_check > 0):
                st.warning(
                    "Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª ØªØµØ§Ø¯ÙÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª ÛŒØ§ Ù‡Ù†ÙˆØ² ØªÙˆÙ„ÛŒØ¯ Ù†Ø´Ø¯Ù‡. Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ Ú©Ù„Ù…Ø§Øª Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒØ¯.")
            elif not final_words_to_process and random_counts_defined:  # Counts were zero
                st.warning(
                    "ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø² Ù„ÛŒØ³Øª ØªØµØ§Ø¯ÙÛŒ ØµÙØ± Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡ Ø§Ø³Øª.")
            elif not random_counts_defined and not final_words_to_process:  # Should not happen if "ØªØµØ§Ø¯ÙÛŒ" is selected
                st.warning("Ù„Ø·ÙØ§ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª ØªØµØ§Ø¯ÙÛŒ Ø±Ø§ Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯.")

        # <-- Ú©Ù„ÛŒØ¯ Ø¬Ø¯ÛŒØ¯ Ø§ÛŒÙ†Ø¬Ø§ Ù‡Ù… Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
        elif word_source_key in ["Ù…Ù†ØªØ®Ø¨ Ø·ÙˆÙ„Ø§Ù†ÛŒ", "Ù…Ù†ØªØ®Ø¨ ÛµÛ°Û°", "Ù…Ù†ØªØ®Ø¨ Û±Û°Û° Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡"]:
            if not words_to_check_from_source:
                st.error(
                    f"Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª {word_source_options[word_source_key]} Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯Ù‡ ÛŒØ§ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª.")
            else:
                final_words_to_process.extend(words_to_check_from_source)

        if final_words_to_process:
            st.subheader(
                f"â³ Ù†ØªØ§ÛŒØ¬ Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø³ÙˆÙ†Ø¯ {domain_extension_val} (Ø§Ø² Ù„ÛŒØ³Øª: {word_source_options[word_source_key]})")

            results_data = []
            total_words = len(final_words_to_process)
            progress_bar_check = st.progress(0)
            status_text_check = st.empty()
            processed_count_placeholder = st.empty()

            for idx, word_prefix in enumerate(final_words_to_process):
                full_domain = word_prefix + domain_extension_val
                status = check_domain_availability(full_domain)
                results_data.append({
                    "Ø±Ø¯ÛŒÙ": idx + 1,
                    "Ú©Ù„Ù…Ù‡": word_prefix,
                    "Ø¯Ø§Ù…Ù†Ù‡ Ú©Ø§Ù…Ù„": full_domain,
                    "ÙˆØ¶Ø¹ÛŒØª": status,
                    "Ø·ÙˆÙ„ Ú©Ù„Ù…Ù‡": len(word_prefix)
                })

                progress = (idx + 1) / total_words
                progress_bar_check.progress(progress)
                status_text_check.text(f"Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ: {full_domain}")
                processed_count_placeholder.markdown(
                    f"**ØªØ¹Ø¯Ø§Ø¯ Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø¯Ù‡: {idx + 1} Ø§Ø² {total_words}**")

            progress_bar_check.empty()
            status_text_check.success(f"âœ… Ø¨Ø±Ø±Ø³ÛŒ {total_words} Ø¯Ø§Ù…Ù†Ù‡ Ú©Ø§Ù…Ù„ Ø´Ø¯.")
            processed_count_placeholder.empty()

            if results_data:
                results_df = pd.DataFrame(results_data)

                st.markdown("---")
                st.subheader("ğŸ“Š Ù†Ù…Ø§ÛŒØ´ Ùˆ Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ù†ØªØ§ÛŒØ¬ Ú©Ù„ÛŒ")
                col_sort1, col_sort2 = st.columns(2)
                sort_by_column = col_sort1.selectbox(
                    "Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ù†ØªØ§ÛŒØ¬ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³ØªÙˆÙ†:",
                    options=["Ø±Ø¯ÛŒÙ", "Ú©Ù„Ù…Ù‡", "Ø¯Ø§Ù…Ù†Ù‡ Ú©Ø§Ù…Ù„",
                             "ÙˆØ¶Ø¹ÛŒØª", "Ø·ÙˆÙ„ Ú©Ù„Ù…Ù‡"],
                    index=0, key="sort_col_main"
                )
                sort_ascending = col_sort2.selectbox(
                    "ØªØ±ØªÛŒØ¨ Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ:",
                    options=["ØµØ¹ÙˆØ¯ÛŒ", "Ù†Ø²ÙˆÙ„ÛŒ"],
                    index=0, key="sort_order_main"
                )
                is_ascending = True if sort_ascending == "ØµØ¹ÙˆØ¯ÛŒ" else False

                if sort_by_column in ["Ø±Ø¯ÛŒÙ", "Ø·ÙˆÙ„ Ú©Ù„Ù…Ù‡"]:
                    results_df[sort_by_column] = pd.to_numeric(
                        results_df[sort_by_column])
                sorted_results_df = results_df.sort_values(
                    by=sort_by_column, ascending=is_ascending)

                st.dataframe(sorted_results_df, use_container_width=True, height=min(
                    35 * (len(sorted_results_df) + 1), 600))

                available_domains_summary_df = results_df[results_df["ÙˆØ¶Ø¹ÛŒØª"] == "Available"].copy(
                )
                if not available_domains_summary_df.empty:
                    available_domains_summary_df.reset_index(
                        drop=True, inplace=True)
                    available_domains_summary_df['Ø±Ø¯ÛŒÙ Ù†Ù…Ø§ÛŒØ´'] = available_domains_summary_df.index + 1
                    st.success(
                        f"ğŸ‰ ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø§Ù…Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ø§Ø¯ ÛŒØ§ÙØª Ø´Ø¯Ù‡: {len(available_domains_summary_df)}")
                    with st.expander("Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù„ÛŒØ³Øª Ø¯Ø§Ù…Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ø§Ø¯ (Ù‚Ø§Ø¨Ù„ Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ Ú©Ù„ÛŒÚ© Ø±ÙˆÛŒ Ù‡Ø¯Ø± Ø³ØªÙˆÙ†)"):
                        st.dataframe(available_domains_summary_df[[
                                     'Ø±Ø¯ÛŒÙ Ù†Ù…Ø§ÛŒØ´', 'Ú©Ù„Ù…Ù‡', 'Ø¯Ø§Ù…Ù†Ù‡ Ú©Ø§Ù…Ù„']], key='df_available', use_container_width=True)
                else:
                    st.info("â„¹ï¸ Ù‡ÛŒÚ† Ø¯Ø§Ù…Ù†Ù‡ Ø¢Ø²Ø§Ø¯ÛŒ Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø±Ø±Ø³ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")

                registered_domains_summary_df = results_df[results_df["ÙˆØ¶Ø¹ÛŒØª"] == "Registered"].copy(
                )
                if not registered_domains_summary_df.empty:
                    registered_domains_summary_df.reset_index(
                        drop=True, inplace=True)
                    registered_domains_summary_df['Ø±Ø¯ÛŒÙ Ù†Ù…Ø§ÛŒØ´'] = registered_domains_summary_df.index + 1
                    st.error(
                        f"ğŸš« ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø§Ù…Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø«Ø¨Øª Ø´Ø¯Ù‡ ÛŒØ§ÙØª Ø´Ø¯Ù‡: {len(registered_domains_summary_df)}")
                    with st.expander("Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù„ÛŒØ³Øª Ø¯Ø§Ù…Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø«Ø¨Øª Ø´Ø¯Ù‡ (Ù‚Ø§Ø¨Ù„ Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ Ú©Ù„ÛŒÚ© Ø±ÙˆÛŒ Ù‡Ø¯Ø± Ø³ØªÙˆÙ†)"):
                        st.dataframe(registered_domains_summary_df[[
                                     'Ø±Ø¯ÛŒÙ Ù†Ù…Ø§ÛŒØ´', 'Ú©Ù„Ù…Ù‡', 'Ø¯Ø§Ù…Ù†Ù‡ Ú©Ø§Ù…Ù„']], key='df_registered', use_container_width=True)
                else:
                    st.info("â„¹ï¸ Ù‡ÛŒÚ† Ø¯Ø§Ù…Ù†Ù‡ Ø«Ø¨Øª Ø´Ø¯Ù‡â€ŒØ§ÛŒ Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø±Ø±Ø³ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")

                saved_filename = save_results_to_json(
                    results_df, domain_extension_val, price_val, list_type_id_for_save)
                st.markdown(
                    f"ğŸ’¾ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± ÙØ§ÛŒÙ„ `{saved_filename}` Ø°Ø®ÛŒØ±Ù‡/Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯.")

        elif word_source_key == "ØªØµØ§Ø¯ÙÛŒ":
            # This condition is a bit tricky now due to locals() check.
            # We need to ensure that if "ØªØµØ§Ø¯ÙÛŒ" was chosen, but counts were zero, a message is shown.
            # The above checks inside the "ØªØµØ§Ø¯ÙÛŒ" block when populating final_words_to_process should cover this.
            # This outer elif might be redundant or needs refinement.
            if 'three_letter_count_to_check' in locals() and 'four_letter_count_to_check' in locals() and \
                    not (three_letter_count_to_check > 0 or four_letter_count_to_check > 0):
                st.warning(
                    "Ù„Ø·ÙØ§Ù‹ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ (Û³ Ø­Ø±ÙÛŒ ÛŒØ§ Û´ Ø­Ø±ÙÛŒ) Ø±Ø§ Ø¨ÛŒØ´ØªØ± Ø§Ø² ØµÙØ± Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯.")

        elif not final_words_to_process and word_source_key != "ØªØµØ§Ø¯ÙÛŒ":
            st.error(
                f"Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª '{word_source_options[word_source_key]}' Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª.")


if __name__ == "__main__":
    main()
